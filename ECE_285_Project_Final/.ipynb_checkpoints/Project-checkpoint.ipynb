{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gram Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b, c, h, w = input.size()\n",
    "        F = input.view(b, c, h * w)\n",
    "        G = torch.bmm(F, F.transpose(1, 2))\n",
    "        G.div_(h * w)\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style transfer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferNetwork(object):\n",
    "    def __init__(self, content,  multi_styles):\n",
    "        super(StyleTransferNetwork, self).__init__()\n",
    "        self.num_of_convs_for_style = 5    # change this to account for the number of convolution layers used for style\n",
    "        self.num_styles = len(multi_styles) # the number of styles we want to transfer to content\n",
    "        self.mult_style_weight = [0] * self.num_styles  # weights according to average weighting of style loss\n",
    "        self.styles = multi_styles\n",
    "        self.log = 0\n",
    "        self.content = content\n",
    "\n",
    "        #self.pastiche = Variable((torch.randn(content.size())).type_as(content.data), requires_grad=True) # use for\n",
    "        # random initialization\n",
    "        self.pastiche = Variable(content.data.clone(), requires_grad=True) # use this to initialize pastiche to content\n",
    "\n",
    "        self.content_layers = ['conv_4_2']\n",
    "        self.style_layers = []\n",
    "\n",
    "        #self.style_layers = ['conv_1_1', 'conv_1_2', 'conv_2_1', 'conv_2_2', 'conv_3_1'] # if you want to manually\n",
    "        # change the layers used for style loss uncomment this line and comment out the for loop in next line\n",
    "\n",
    "        for i in range(0, self.num_of_convs_for_style):\n",
    "             self.style_layers.append(\"conv_\" + str(i+1) + \"_1\")\n",
    "\n",
    "        #self.style_loss_weight = [1.0 / (1000 * self.num_of_convs_for_style)] * self.num_of_convs_for_style # uncomment\n",
    "        # this line if random initialization is used\n",
    "        self.style_loss_weight = [1.0 / (1000*self.num_of_convs_for_style)] * self.num_of_convs_for_style\n",
    "\n",
    "        #self.style_loss_weight = [1 / (self.num_of_convs_for_style*n**2) for n in [64, 128, 256, 512, 512]]\n",
    "        self.content_weight = 1     # this is alpha according to the paper\n",
    "        self.style_weight = 1000    # this is beta according to paper\n",
    "\n",
    "        self.loss_network = models.vgg19(pretrained=True)\n",
    "        for param in self.loss_network.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.gram = GramMatrix()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.LBFGS([self.pastiche])\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.loss_network.cuda()\n",
    "            self.gram.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        def closure():\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            pastiche = self.pastiche\n",
    "\n",
    "            # the following two lines are used for clamping the pastiche to be between 1 and -1, uncomment when pastiche\n",
    "            # is initialized to random noise\n",
    "            # pastiche.data[pastiche.data > 1] = 1\n",
    "            # pastiche.data[pastiche.data < -1] = -1\n",
    "\n",
    "\n",
    "            content = self.content.clone()\n",
    "            styles_to_transfer = []\n",
    "            for z in range(0, self.num_styles):\n",
    "                styles_to_transfer.append(self.styles[z].clone())\n",
    "            styles_loss = torch.zeros(self.num_styles).type(dtype)\n",
    "            content_loss = 0\n",
    "            j = 0\n",
    "            i = 1\n",
    "            layer_count = 1\n",
    "            not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "            for layer in list(self.loss_network.features):\n",
    "                layer = not_inplace(layer)\n",
    "                if self.use_cuda:\n",
    "                    layer.cuda()\n",
    "\n",
    "                pastiche, content = layer.forward(pastiche), layer.forward(content)\n",
    "\n",
    "                for z in range(0, self.num_styles):\n",
    "                    styles_to_transfer[z] = layer.forward(styles_to_transfer[z])\n",
    "\n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    name = \"conv_\" + str(layer_count) + \"_\" + str(i)\n",
    "                    if name in self.content_layers:\n",
    "                        content_loss += self.loss(pastiche, content.detach())\n",
    "\n",
    "                    if name in self.style_layers:\n",
    "                        pastiche_g = self.gram.forward(pastiche)\n",
    "                        for z in range(0, self.num_styles):\n",
    "                            style_g = self.gram.forward(styles_to_transfer[z])\n",
    "                            styles_loss[z] += self.style_loss_weight[j] * self.loss(pastiche_g,\n",
    "                                                                                    style_g.detach())\n",
    "                        j += 1\n",
    "                if isinstance(layer, nn.ReLU):\n",
    "                    i += 1\n",
    "                if isinstance(layer, nn.MaxPool2d):\n",
    "                    layer_count += 1\n",
    "                    i = 1\n",
    "            tot_style_loss = sum(styles_loss)\n",
    "            total_loss = self.content_weight * content_loss\n",
    "            for z in range(0, self.num_styles):\n",
    "\n",
    "                #self.mult_style_weight[z] = 1.0 / self.num_styles  # uncomment this line and comment out next\n",
    "                # if custom (equal in this case) weight is needed for each style\n",
    "\n",
    "                self.mult_style_weight[z] = styles_loss[z]/tot_style_loss # comment out if custom weights for each style\n",
    "                # is needed\n",
    "                total_loss += self.style_weight * self.mult_style_weight[z] * styles_loss[z]\n",
    "            total_loss.backward()\n",
    "            self.log = total_loss\n",
    "            return total_loss\n",
    "\n",
    "        self.optimizer.step(closure)\n",
    "        return self.pastiche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 256\n",
    "\n",
    "mean_imgnet = [123.68/255, 116.779/255, 103.939/255]\n",
    "neg_mean_imgnet = [-123.68/255, -116.779/255, -103.939/255]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pre_process = transforms.Compose([transforms.Resize((imsize, imsize)),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=mean_imgnet, # subtract imagenet mean and normalize\n",
    "                                                std=[1,1,1]),\n",
    "                                  transforms.Lambda(lambda x: x.mul_(255)),  # to have the range from [-255, 255]\n",
    "                                  ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Image Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_process_a = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1. / 255)),  # convert back from [-255, 255]\n",
    "                                     transforms.Normalize(mean=neg_mean_imgnet, # add imagenet mean back\n",
    "                                                std=[1,1,1]),\n",
    "                                     ])\n",
    "\n",
    "\n",
    "post_process_b = transforms.Compose([transforms.ToPILImage()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postp(tensor):\n",
    "    t = post_process_a(tensor)\n",
    "    t[t > 1] = 1 # to clamp results in the range [0,1]\n",
    "    t[t < 0] = 0 # to clamp results in the range [0,1]\n",
    "    img = post_process_b(t)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_styles(path):\n",
    "    styles = []\n",
    "    for file in glob.glob(path+\"/*.jpg\"):\n",
    "        styles.append(image_loader(file).type(dtype))\n",
    "\n",
    "    return styles\n",
    "\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = pre_process(image)\n",
    "    image = Variable(image.unsqueeze(0))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    path_styles = \"./styles\"\n",
    "    path_contents = \"./contents\"\n",
    "    out_path = \"./outputs\"\n",
    "    num_epochs = 50\n",
    "    styles = load_styles(path_styles)\n",
    "    for cont_file in glob.glob(path_contents + \"/*.jpg\"):\n",
    "        log = []\n",
    "        content = image_loader(cont_file).type(dtype)\n",
    "\n",
    "        style_cnn = StyleTransferNetwork(content, styles)\n",
    "        out_counter = 0\n",
    "        for i in range(num_epochs + 1):\n",
    "            pastiche = style_cnn.train()\n",
    "            print(str(style_cnn.log.item()) + \" \" + str(i))\n",
    "            log.append([i, style_cnn.log.item()])\n",
    "            if i % 10 == 0:\n",
    "                print(\"Iteration: %d\" % i)\n",
    "                cont_name = cont_file.replace('./contents\\\\', \"\")\n",
    "                cont_name = cont_name.replace(\".jpg\", \"\")\n",
    "                path = out_path + \"/\" + cont_name + \"_%d.png\" % out_counter\n",
    "                out_img = postp(pastiche.data[0].cpu().squeeze())\n",
    "                scipy.misc.imsave(path, out_img)\n",
    "                out_counter += 1\n",
    "        a = np.array(log)\n",
    "        plt.figure(2)\n",
    "        plt.plot(a[1:, 0], np.log(a[1:, 1]))\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('loss(log)')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-9cf222e221ee>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mout_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mpastiche\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstyle_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-1667058a3341>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpastiche\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0morig_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-1667058a3341>\u001b[0m in \u001b[0;36mclosure\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m                     \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[0mpastiche\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpastiche\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_styles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 301\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
